{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "Over the next two weeks, we will studying (artificial) neural networks with multiple interconnected neurons and write some code to make them work. Neural networks excel at machine learning tasks when large amounts of data is available (thousands of datapoints).\n",
    "\n",
    "This week's notes will primarily be based on <a href=\"http://neuralnetworksanddeeplearning.com/\">*Neural Networks and Deep Learning*</a> by Michael Nielsen.\n",
    "\n",
    "This <a href=\"https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\">playlist of YouTube videos</a> is based on Nielsen's book and is pretty amazing for explaining this content as well. I cannot recommend it enough!\n",
    "\n",
    "<a href=\"https://github.com/mnielsen/neural-networks-and-deep-learning\">Nielsen's code</a> is written in Python 2.7, so I would not recommend using his code, but we will write some new code in Python 3.7.\n",
    "\n",
    "# Lecture 5 - Sept 7 - Feedforward Neural Networks\n",
    "\n",
    "The approach to making neural networks learn is very similar to the gradient-based learning approach we used previously, but the scale of large neural networks requires us to adjust certain parts of it. There are four main bits of code we need to make neural nets work:\n",
    "\n",
    "1. **Forward pass**: feeds a datapoint into a neural network and processes it by computing the outputs of each layer of artificial neurons until we reach the output layer and compute the loss\n",
    "\n",
    "2. **Loss**: after feeding in some labeled data, we can compute the loss function measuring the network's training error.\n",
    "\n",
    "3. **Backpropagation (Backprop)**: uses the chain rule from calculus in a systematic way to find the gradient of the loss function.\n",
    "\n",
    "4. **Gradient descent**: at each iteration, nudge the weights in the opposite direction of the gradient.\n",
    "\n",
    "##### Note: *Stochastic* Gradient Descent (SGD)\n",
    "\n",
    "We will discuss and add SGD next week. It speeds up gradient descent by making weight updates more frequently--based on samples of the training data rather than the entire dataset. SGD is critical for large neural networks to operate properly, but we will avoid scaling up too much this week.\n",
    "\n",
    "### The Forward Pass\n",
    "\n",
    "The forward pass is simple, we just follow the simple rules of the individual neurons, but we have to do it **many** times, which we can automate with matrix multiplication.\n",
    "\n",
    "### Loss\n",
    "\n",
    "The loss function can be computed quite simply in the same way as we did last week. However, we may want to add weight decay or other features to the loss function or we may want to use a different loss function entirely.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "In the past, we approximated the gradient by computing the loss functions many times, perturbed in each dimension to compute\n",
    "\n",
    "$$\\nabla L(w)\\approx\\left(\\frac{L(w+he_0)-L(w)}{h}, ..., \\frac{L(w+he_d)-L(w)}{h}\\right)$$\n",
    "\n",
    "for some small value $h$, but this meant we had to compute the loss function $d+1$ times for **each** iteration in gradient descent, which required us to compute the outputs of the model for the whole dataset many, many times. This was fine for the linear regression models we have considered, but it would be too computationally expensive to feed the whole dataset through a neural network so many times.\n",
    "\n",
    "As a result, we instead need another method to compute gradients. It turns out, we can systematically apply the chain rule to get formulas for the partial derivatives of the loss function with respect to each weight in the neural network. Backpropagation does just this.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "As we have seen before, gradient descent is an iterative learning algorithm that aims to minimize the loss function. It iteratively nudges the weights in the opposite direction of the gradients of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6 - Sept 9 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See details in the lecture notes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
